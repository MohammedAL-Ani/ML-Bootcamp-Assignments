{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 08: Advanced Sequence Applications\n",
        "\n",
        "This Week's assignment is to train sequence models on the `Cafe Chatbot Data`.\n",
        "\n",
        "Before starting copy this file and work on your own copy by following the below steps: <br>\n",
        "`File > Save Copy in Drive`. Then add your name to the file (e.g., Assignment 07: Sequence Modeling - Zahraa Dhafer).\n",
        "\n",
        "**DATASET**\n",
        "This dataset consists of three csv files, you'll be working on the conversationo CSV file which contains two columns of questions and answers.<br><br>\n",
        "**Submission Deadline: Saturday, 3/12/2022 at 3:00 PM**\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "1. Import all necessary libraries for the sequence modeling project.\n",
        "2. Download the dataset (the link is provided below).\n",
        "3. Read data from CSV file.\n",
        "4. Prepare the data:\n",
        "*   Clean the sentences by removing special characters.\n",
        "*   Add a start and end token to each sentence.\n",
        "*   Tokenize the sentences.\n",
        "*   Pad each sentence to a maximum length.\n",
        "<br>\n",
        "5. Create the Data Pipeline for the Model\n",
        "6. Create the Seq2Seq Model Architecture.<br>\n",
        "**Note**: Use Adam optimizer and the appropriate loss function.\n",
        "7. Create the Optimizer and the Loss Function.\n",
        "8. Create the Training Step.\n",
        "9. Create the Training Loop\n",
        "\n",
        "**Note:** you can overfit your model.<br>\n",
        "**HINTS:**\n",
        "Set the new hyperparameters like vocabulary size, input length (i.e. max sequence length) in a separate cell after the import cell in your notebook (failing to do so will affect your style score)\n",
        "\n",
        "**Note:** To get the best performance from the model, manually tune the hyperparameters of the model. \n",
        "\n",
        "Find relevant links below:<br>\n",
        "\n",
        "[Assignment Colab File](https://colab.research.google.com/drive/1N6IcInQFF0iJdl8pruUvb4-rAacE0IBC?usp=sharing)<br>\n",
        "\n",
        "[Dataset](https://www.kaggle.com/sonalibhoir/cafe-chatbot-dataset?select=conversationo.csv)\n",
        "\n",
        "[Submission Form](https://docs.google.com/forms/d/e/1FAIpQLSf-WkcxjmZcxdUcu5OG7ZxFOsQu-qBy-u1UFNPRkWmjeW7w3g/viewform?usp=pp_url)<br>\n",
        "\n",
        "\n",
        "Good luck and feel free to ask any questions in the or on the Questions channel."
      ],
      "metadata": {
        "id": "AU78WMPocdFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm /content/conversationo.csv\n",
        "!rm /content/food.csv\n",
        "!rm /content/Item_to_id.csv"
      ],
      "metadata": {
        "id": "n5RRndktjXIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- Import all necessary libraries for the sequence modeling project"
      ],
      "metadata": {
        "id": "NY3o4kreqOLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "from sklearn import model_selection\n"
      ],
      "metadata": {
        "id": "0nYUDKQBic7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buffer_size=937\n",
        "vocab_size=64\n",
        "\n",
        "units=300\n",
        "batch_size=32\n",
        "embedding_dim = 256\n",
        "\n",
        "# units = 1024"
      ],
      "metadata": {
        "id": "rekmLMw_BqxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Download the dataset"
      ],
      "metadata": {
        "id": "dG-8I4sUqatO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "\n",
        "!unzip /content/archive.zip"
      ],
      "metadata": {
        "id": "TPmmLNyHaq8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc0d9af9-af1a-42db-853f-8b8bbd4cc8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/archive.zip\n",
            "  inflating: Item_to_id.csv          \n",
            "  inflating: conversationo.csv       \n",
            "  inflating: food.csv                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2kRRAX6ZjKu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Read data from CSV file"
      ],
      "metadata": {
        "id": "MLA4G5gFqjOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "# write your code below\n",
        "pf=pd.read_csv('/content/conversationo.csv')\n",
        "df=DataFrame(pf)\n",
        "df"
      ],
      "metadata": {
        "id": "U5jS8nESqtmf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "3881e8e1-64ed-4016-9d43-d604de978aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     Question  \\\n",
              "0                                         hey   \n",
              "1                            do u have coffee   \n",
              "2    i will take one espresso and 5 americano   \n",
              "3                            anything special   \n",
              "4                           suggest something   \n",
              "..                                        ...   \n",
              "974           what is price of French Coffee    \n",
              "975         what is price of Iced Coffee Late   \n",
              "976          what is price of Latte Macchiato   \n",
              "977     what is price of Wainans Choco Coffee   \n",
              "978                           book me a table   \n",
              "\n",
              "                                                answer  \n",
              "0                           Hello! How may I help you.  \n",
              "1    Yes sir  Simple Coffee ,Cappuchino, Americano,...  \n",
              "2    Sir thanks for your order. You have ordered 1 ...  \n",
              "3    We have coffe,pastries,puff pastries and milks...  \n",
              "4    We have coffe,pastries,puff pastries and milks...  \n",
              "..                                                 ...  \n",
              "974  Its our one of best, you can enjoy it at just ...  \n",
              "975  Its our one of best, you can enjoy it at just ...  \n",
              "976  Its our one of best, you can enjoy it at just ...  \n",
              "977  Its our one of best, you can enjoy it at just ...  \n",
              "978  To book a table you can click on last icon on ...  \n",
              "\n",
              "[979 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41b6d761-0921-4b53-a3f5-6c0eb4ae6bb1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hey</td>\n",
              "      <td>Hello! How may I help you.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>do u have coffee</td>\n",
              "      <td>Yes sir  Simple Coffee ,Cappuchino, Americano,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i will take one espresso and 5 americano</td>\n",
              "      <td>Sir thanks for your order. You have ordered 1 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>anything special</td>\n",
              "      <td>We have coffe,pastries,puff pastries and milks...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>suggest something</td>\n",
              "      <td>We have coffe,pastries,puff pastries and milks...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>what is price of French Coffee</td>\n",
              "      <td>Its our one of best, you can enjoy it at just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>what is price of Iced Coffee Late</td>\n",
              "      <td>Its our one of best, you can enjoy it at just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>what is price of Latte Macchiato</td>\n",
              "      <td>Its our one of best, you can enjoy it at just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>what is price of Wainans Choco Coffee</td>\n",
              "      <td>Its our one of best, you can enjoy it at just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>book me a table</td>\n",
              "      <td>To book a table you can click on last icon on ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>979 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41b6d761-0921-4b53-a3f5-6c0eb4ae6bb1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-41b6d761-0921-4b53-a3f5-6c0eb4ae6bb1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-41b6d761-0921-4b53-a3f5-6c0eb4ae6bb1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGoX6vcHhkZA",
        "outputId": "3ec94a5e-232b-4f51-c7bd-bd33dde809ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 979 entries, 0 to 978\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Question  961 non-null    object\n",
            " 1   answer    939 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 15.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(inplace=True)\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYVyAFE9hpuv",
        "outputId": "129e2474-8b23-4fde-fe7b-3db3a7b0f732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 937 entries, 0 to 978\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   Question  937 non-null    object\n",
            " 1   answer    937 non-null    object\n",
            "dtypes: object(2)\n",
            "memory usage: 22.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.Question"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5X85Io4iwGn",
        "outputId": "13fbfa91-5541-401b-cae3-b250f7cf0184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                           hey\n",
              "1                              do u have coffee\n",
              "2      i will take one espresso and 5 americano\n",
              "3                              anything special\n",
              "4                             suggest something\n",
              "                         ...                   \n",
              "974             what is price of French Coffee \n",
              "975           what is price of Iced Coffee Late\n",
              "976            what is price of Latte Macchiato\n",
              "977       what is price of Wainans Choco Coffee\n",
              "978                             book me a table\n",
              "Name: Question, Length: 937, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4- Prepare the data:\n"
      ],
      "metadata": {
        "id": "MbkGa2lwrZ-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clean the sentences by removing special characters"
      ],
      "metadata": {
        "id": "6WfgTE72aQPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub(r'[^\\w]',' ',text)\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    text = \" \".join(filter(lambda x:x[0]!=\"@\", text.split()))\n",
        "    return text\n",
        "df.Question=df.Question.map(clean_text)\n",
        "df.answer =df.answer.map(clean_text)\n"
      ],
      "metadata": {
        "id": "WVj4OxmprE7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add a start and end token to each sentence"
      ],
      "metadata": {
        "id": "hoQcvUo2ahWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "def startend(text):\n",
        "  text=f'<start> {text} <end>'\n",
        "  return text\n",
        "\n",
        "df.Question=df.Question.map(startend)\n",
        "df.answer =df.answer.map(startend)\n",
        "df"
      ],
      "metadata": {
        "id": "nwpRSMy_ahWS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "248f642c-9a28-4c42-ff41-8918c8178cc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Question  \\\n",
              "0                                    <start> hey <end>   \n",
              "1                       <start> do u have coffee <end>   \n",
              "2    <start> i will take one espresso and americano...   \n",
              "3                       <start> anything special <end>   \n",
              "4                      <start> suggest something <end>   \n",
              "..                                                 ...   \n",
              "974       <start> what is price of french coffee <end>   \n",
              "975    <start> what is price of iced coffee late <end>   \n",
              "976     <start> what is price of latte macchiato <end>   \n",
              "977  <start> what is price of wainans choco coffee ...   \n",
              "978                      <start> book me a table <end>   \n",
              "\n",
              "                                                answer  \n",
              "0               <start> hello how may i help you <end>  \n",
              "1    <start> yes sir simple coffee cappuchino ameri...  \n",
              "2    <start> sir thanks for your order you have ord...  \n",
              "3    <start> we have coffe pastries puff pastries a...  \n",
              "4    <start> we have coffe pastries puff pastries a...  \n",
              "..                                                 ...  \n",
              "974  <start> its our one of best you can enjoy it a...  \n",
              "975  <start> its our one of best you can enjoy it a...  \n",
              "976  <start> its our one of best you can enjoy it a...  \n",
              "977  <start> its our one of best you can enjoy it a...  \n",
              "978  <start> to book a table you can click on last ...  \n",
              "\n",
              "[937 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-76d9badb-abd8-4382-97a9-9a5f1b2d405e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&lt;start&gt; hey &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; hello how may i help you &lt;end&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>&lt;start&gt; do u have coffee &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; yes sir simple coffee cappuchino ameri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;start&gt; i will take one espresso and americano...</td>\n",
              "      <td>&lt;start&gt; sir thanks for your order you have ord...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;start&gt; anything special &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; we have coffe pastries puff pastries a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;start&gt; suggest something &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; we have coffe pastries puff pastries a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>&lt;start&gt; what is price of french coffee &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; its our one of best you can enjoy it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>&lt;start&gt; what is price of iced coffee late &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; its our one of best you can enjoy it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>&lt;start&gt; what is price of latte macchiato &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; its our one of best you can enjoy it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>&lt;start&gt; what is price of wainans choco coffee ...</td>\n",
              "      <td>&lt;start&gt; its our one of best you can enjoy it a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>&lt;start&gt; book me a table &lt;end&gt;</td>\n",
              "      <td>&lt;start&gt; to book a table you can click on last ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>937 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76d9badb-abd8-4382-97a9-9a5f1b2d405e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76d9badb-abd8-4382-97a9-9a5f1b2d405e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76d9badb-abd8-4382-97a9-9a5f1b2d405e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize the sentences"
      ],
      "metadata": {
        "id": "pmVlBOehaiMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "def token_texts(texts):\n",
        "  token_zer=tf.keras.preprocessing.text.Tokenizer(oov_token='OOV',\n",
        "                                              filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
        "  token_zer.fit_on_texts(texts)\n",
        "  tensor=token_zer.texts_to_sequences(texts)\n",
        "\n",
        "  return tensor,token_zer\n",
        "\n",
        "tensor_Q,token_Q=token_texts(df.Question)\n",
        "tensor_a,token_a = token_texts(df.answer)\n"
      ],
      "metadata": {
        "id": "Y9acRrtbaiMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pad each sentence to a maximum length"
      ],
      "metadata": {
        "id": "H_Sd4aQGasLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "def padding_text(texts):\n",
        "  \n",
        "  pad=tf.keras.preprocessing.sequence.pad_sequences(sequences=texts,padding='post')\n",
        "  return pad\n",
        "padd_Q=padding_text(tensor_Q)\n",
        "padd_a=padding_text(tensor_a)\n"
      ],
      "metadata": {
        "id": "3bOcWmYyasLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padd_a.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMGC-VhEIQyc",
        "outputId": "ecd110e8-9ed3-4117-eb77-1925bed4cf2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(937, 54)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_sequence_Q=padd_Q.shape[1]\n",
        "max_sequence_a=padd_a.shape[1]\n"
      ],
      "metadata": {
        "id": "IWkOjKY0IZj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token_Q.word_index['hey']\n",
        "tensor_a[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOBStM7KqG53",
        "outputId": "c20de7d2-35e6-4483-dc3c-afc15ed4b949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2, 47, 48, 23, 22, 49, 4, 3]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5- Create the Data Pipeline for the Model"
      ],
      "metadata": {
        "id": "K5x6ahnDZZfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train,x_test,y_train,y_test=model_selection.train_test_split(padd_Q,padd_a,random_state=42,\n",
        "#                                                 test_size=0.1)"
      ],
      "metadata": {
        "id": "jzXVqKl9sbgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# x_train.shape"
      ],
      "metadata": {
        "id": "i5KKDbrytrLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "def create_data(x,y):\n",
        "  data=tf.data.Dataset.from_tensor_slices((x,y))\n",
        "  data=data.shuffle(buffer_size=buffer_size)\n",
        "  data=data.batch(batch_size=batch_size,drop_remainder=True)\n",
        "  data=data.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "  return data\n",
        "train_dataset=create_data(padd_Q,padd_a)\n",
        "# test_dataset=create_data(x_test,y_test)\n"
      ],
      "metadata": {
        "id": "OIrFkw5-Zfoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the first batch of the training data \n",
        "for question, answer in train_dataset.take(1):\n",
        "  print(f\"question:{question.shape}\\n{question}\")\n",
        "\n",
        "  print(f\"answer:{answer.shape}\\n{answer}\")\n",
        "  question_sample = question\n",
        "  answer_sample = answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Xms6dWauAz6",
        "outputId": "cd347418-7b10-47a4-a0a7-d46fd4cd94f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "question:(32, 49)\n",
            "[[ 2  4  6 ...  0  0  0]\n",
            " [ 2  4  7 ...  0  0  0]\n",
            " [ 2 35  3 ...  0  0  0]\n",
            " ...\n",
            " [ 2  4 14 ...  0  0  0]\n",
            " [ 2  4  7 ...  0  0  0]\n",
            " [ 2 16 25 ...  0  0  0]]\n",
            "answer:(32, 54)\n",
            "[[ 2 20  6 ...  0  0  0]\n",
            " [ 2 10  7 ...  0  0  0]\n",
            " [ 2 47 48 ...  0  0  0]\n",
            " ...\n",
            " [ 2 20  6 ...  0  0  0]\n",
            " [ 2 10  7 ...  0  0  0]\n",
            " [ 2 24 50 ...  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create nesseary variables\n",
        "# the size of the vocabulary for the question and the answer\n",
        "vocab_inp_size= len(token_Q.word_index)+1\n",
        "vocab_tar_size=  len(token_a.word_index)+1\n",
        "print(vocab_inp_size)\n",
        "print(vocab_tar_size )\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzsV7FbPT-3L",
        "outputId": "39389ef3-5310-4fd9-9733-546dfab6260c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "166\n",
            "201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6- Create the Seq2Seq Model Architecture"
      ],
      "metadata": {
        "id": "nOEo3I9grsab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder Model"
      ],
      "metadata": {
        "id": "EWC9UWmbZ5a6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,encoder_units,batch_size):\n",
        "      super(Encoder,self).__init__()\n",
        "\n",
        "      self.encoder_units=encoder_units\n",
        "\n",
        "      self.embedding=tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
        "                                               mask_zero=True)\n",
        "      self.batch_size=batch_size\n",
        "\n",
        "      self.gru=tf.keras.layers.GRU(units=self.encoder_units,return_sequences=True,\n",
        "                                   return_state=True,recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "  def call(self,x,hidden):\n",
        "    x=self.embedding(x)\n",
        "    out,state=self.gru(x,initial_state=hidden)\n",
        "\n",
        "    return out,state\n",
        "\n",
        "\n",
        "  def initializer_hiddenstate(self):\n",
        "    return tf.zeros(shape=(self.batch_size,self.encoder_units))      \n"
      ],
      "metadata": {
        "id": "Fy6HA5ZIrFLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Model"
      ],
      "metadata": {
        "id": "QdZTOtgxZwb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self,vocab_size,embedding_dim,decoder_units,batch_size):\n",
        "      super(Decoder,self).__init__()\n",
        "\n",
        "      self.decoder_units=decoder_units\n",
        "\n",
        "      self.batch_size=batch_size\n",
        "      \n",
        "\n",
        "      self.embedding=tf.keras.layers.Embedding(vocab_size,output_dim=embedding_dim,mask_zero=True)\n",
        "\n",
        "      self.gru=tf.keras.layers.GRU(units=self.decoder_units,recurrent_initializer='glorot_uniform',\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "      \n",
        "      self.fc=tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "\n",
        "\n",
        "  def call(self,x,hidden):\n",
        "    x=self.embedding(x)\n",
        "    out,state=self.gru(x,hidden)\n",
        "    out=tf.reshape(out,(-1,out.shape[2]))\n",
        "\n",
        "    x=tf.nn.softmax(self.fc(out))\n",
        "\n",
        "    return x,hidden\n"
      ],
      "metadata": {
        "id": "ul17SieaaHuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, batch_size)\n",
        "\n",
        "# sample input\n",
        "# inilize the hidden state with zeros \n",
        "# the shape of hidden is (batch_size, units)\n",
        "sample_hidden = encoder.initializer_hiddenstate()\n",
        "sample_output, sample_hidden = encoder(question, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHsK9AYDssf3",
        "outputId": "54699fc5-63f2-4b41-adef-d2d528b91a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (32, 49, 300)\n",
            "Encoder Hidden state shape: (batch size, units) (32, 300)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder=Decoder(vocab_tar_size,embedding_dim,units,batch_size)\n",
        "\n",
        "sample_decoder_out, _ = decoder(tf.random.uniform(shape=(batch_size,1))\n",
        "                                              ,sample_hidden)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_out.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIQd5OPs8Cf7",
        "outputId": "85be1f6c-63d5-45b9-f34e-afc33d37ad41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (32, 201)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7- Create the Optimizer and the Loss Function"
      ],
      "metadata": {
        "id": "r1MUIcM2a3Mp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code below\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "# create the loss function\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False, reduction='none')\n",
        "\n",
        "# define the loss function for the training\n",
        "def loss_function(real, pred):\n",
        "  # create the mask to ignore the padding tokens\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  # mask shape == (batch_size, sequence_length)\n",
        "  # calculate the loss\n",
        "  loss_ = loss_object(real, pred)\n",
        "  # mask the loss\n",
        "  # how the mask works:\n",
        "  # if the value is 1, the loss is calculated\n",
        "  # if the value is 0, the loss is ignored\n",
        "    #[1,1,1,1,1,1,0,0,0,0,0] mask\n",
        "    # *\n",
        "    #[2,6,2,1,6,3,2,1,5,7,9] input\n",
        "    # =\n",
        "    #[2,6,2,1,6,3,0,0,0,0,0] output\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  # mask shape == (batch_size, sequence_length)\n",
        "  \n",
        "  loss_ *= mask\n",
        "  # calculate the average loss per batch \n",
        "  return tf.reduce_mean(loss_)\n"
      ],
      "metadata": {
        "id": "0uosbAXObbMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-CZU3rwDEMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the training metric \n",
        "train_loss = tf.metrics.Mean(name='train loss')\n",
        "# create the testing metric \n",
        "test_loss =tf.metrics.Mean(name='test loss')"
      ],
      "metadata": {
        "id": "6RLhtSbiDBhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8- Create the Training Step"
      ],
      "metadata": {
        "id": "iGYQ9J45a7nN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Gradients Using Autodiff\n",
        "\n"
      ],
      "metadata": {
        "id": "yMpD3SWTEE_V"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uOdQnp5FC99x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@tf.function\n",
        "def train_setp(inputs,target,enc_hidden):\n",
        "  loss=0\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    enc_output , enc_hidden = encoder(inputs,enc_hidden)\n",
        "\n",
        "    dec_hidden= enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([token_a.word_index['<start>']] * inputs.shape[0],1)\n",
        "\n",
        "    for t in range(1,target.shape[1]):\n",
        "      predictions , dec_hidden = decoder(dec_input,dec_hidden)\n",
        "      loss+=loss_function(target[:,t],predictions)\n",
        "      dec_input=tf.expand_dims(target[:,t],1)\n",
        "\n",
        "    batch_loss = (loss/int(target.shape[1]))\n",
        "    variables=encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients=tape.gradient(loss,variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients,variables))\n",
        "\n",
        "    train_loss(batch_loss)\n",
        "    return batch_loss\n",
        "\n"
      ],
      "metadata": {
        "id": "c-MMEEGCbVqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9- Create the Training Loop"
      ],
      "metadata": {
        "id": "1TS48ppybDGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# write your code below\n",
        "epochs = 200\n",
        "\n",
        "old_test_loss = 1000000\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss.reset_states()\n",
        "  test_loss.reset_states()\n",
        "\n",
        "  enc_hidden = encoder.initializer_hiddenstate()\n",
        "  \n",
        "  steps_per_epoch = padd_a.shape[0]//batch_size\n",
        "  target = steps_per_epoch\n",
        "  bar = tf.keras.utils.Progbar(target)\n",
        "\n",
        "  count = 0\n",
        "  for(batch,(input,target)) in enumerate(train_dataset):\n",
        "    count += 1\n",
        "    batch_loss = train_setp(input,target,enc_hidden)\n",
        "    bar.update(count)\n",
        "\n",
        "\n",
        "\n",
        "  if old_test_loss> test_loss.result():\n",
        "        # set the old test loss to the test loss \n",
        "        old_test_loss= test_loss.result()\n",
        "        encoder.save(filepath='/content/models/encoder')\n",
        "        decoder.save(filepath='/content/models/decoder')\n",
        "        print('Model is saved')\n",
        "    # print the training and testing loss\n",
        "  print('>' *50)\n",
        "  print(f'Epoch #{epoch + 1}')\n",
        "  print(f'Training Loss {train_loss.result()}')\n",
        "  print('<' * 50)\n"
      ],
      "metadata": {
        "id": "bUfkQvyJbWA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "921ba456-ef7a-4d13-f94b-6744d8881df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "29/29 [==============================] - 163s 213ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_2_layer_call_fn, gru_cell_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/models/encoder/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/models/encoder/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7fc184ef3b50> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_3_layer_call_fn, gru_cell_3_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/models/decoder/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/models/decoder/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7fc185013810> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is saved\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #1\n",
            "Training Loss 1.0472263097763062\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #2\n",
            "Training Loss 0.696025013923645\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #3\n",
            "Training Loss 0.42190974950790405\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #4\n",
            "Training Loss 0.26402512192726135\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #5\n",
            "Training Loss 0.1761874258518219\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #6\n",
            "Training Loss 0.13048119843006134\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 214ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #7\n",
            "Training Loss 0.10477633029222488\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #8\n",
            "Training Loss 0.0877356231212616\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #9\n",
            "Training Loss 0.07865548133850098\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #10\n",
            "Training Loss 0.07020676881074905\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #11\n",
            "Training Loss 0.06517892330884933\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #12\n",
            "Training Loss 0.060976751148700714\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #13\n",
            "Training Loss 0.0571846105158329\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #14\n",
            "Training Loss 0.0545545369386673\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #15\n",
            "Training Loss 0.05226808041334152\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #16\n",
            "Training Loss 0.049955807626247406\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #17\n",
            "Training Loss 0.04814525693655014\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 213ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #18\n",
            "Training Loss 0.04637235030531883\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #19\n",
            "Training Loss 0.04556095600128174\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #20\n",
            "Training Loss 0.04473159462213516\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #21\n",
            "Training Loss 0.04407200589776039\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #22\n",
            "Training Loss 0.043403517454862595\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #23\n",
            "Training Loss 0.04272712022066116\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #24\n",
            "Training Loss 0.04251829907298088\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #25\n",
            "Training Loss 0.041520703583955765\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #26\n",
            "Training Loss 0.040644194930791855\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #27\n",
            "Training Loss 0.03950623422861099\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #28\n",
            "Training Loss 0.03971317782998085\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #29\n",
            "Training Loss 0.039180997759103775\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #30\n",
            "Training Loss 0.038929376751184464\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #31\n",
            "Training Loss 0.03814114257693291\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #32\n",
            "Training Loss 0.03824375942349434\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #33\n",
            "Training Loss 0.038090795278549194\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #34\n",
            "Training Loss 0.037877704948186874\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #35\n",
            "Training Loss 0.037466004490852356\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #36\n",
            "Training Loss 0.03760434314608574\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #37\n",
            "Training Loss 0.03784793242812157\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #38\n",
            "Training Loss 0.03739668428897858\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #39\n",
            "Training Loss 0.03690364956855774\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #40\n",
            "Training Loss 0.03736240416765213\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #41\n",
            "Training Loss 0.037312909960746765\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #42\n",
            "Training Loss 0.03714761883020401\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #43\n",
            "Training Loss 0.036009080708026886\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #44\n",
            "Training Loss 0.03632410243153572\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #45\n",
            "Training Loss 0.036909449845552444\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #46\n",
            "Training Loss 0.035774994641542435\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #47\n",
            "Training Loss 0.03676799684762955\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #48\n",
            "Training Loss 0.036239806562662125\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #49\n",
            "Training Loss 0.03606727346777916\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #50\n",
            "Training Loss 0.0364760123193264\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #51\n",
            "Training Loss 0.03579171001911163\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 214ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #52\n",
            "Training Loss 0.03637043386697769\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #53\n",
            "Training Loss 0.03682831674814224\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #54\n",
            "Training Loss 0.03626106679439545\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #55\n",
            "Training Loss 0.03573667258024216\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #56\n",
            "Training Loss 0.03601429611444473\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #57\n",
            "Training Loss 0.03609132766723633\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #58\n",
            "Training Loss 0.03685567155480385\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #59\n",
            "Training Loss 0.03618868440389633\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #60\n",
            "Training Loss 0.036473747342824936\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #61\n",
            "Training Loss 0.03619186952710152\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 214ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #62\n",
            "Training Loss 0.03608608618378639\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #63\n",
            "Training Loss 0.035840023308992386\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #64\n",
            "Training Loss 0.03582022339105606\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 214ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #65\n",
            "Training Loss 0.03615490719676018\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #66\n",
            "Training Loss 0.036106858402490616\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #67\n",
            "Training Loss 0.035280223935842514\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #68\n",
            "Training Loss 0.03602980077266693\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #69\n",
            "Training Loss 0.036378633230924606\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #70\n",
            "Training Loss 0.0356830358505249\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #71\n",
            "Training Loss 0.036331694573163986\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #72\n",
            "Training Loss 0.03610090911388397\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #73\n",
            "Training Loss 0.03576742485165596\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #74\n",
            "Training Loss 0.0357348695397377\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #75\n",
            "Training Loss 0.03633198142051697\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #76\n",
            "Training Loss 0.0360712893307209\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #77\n",
            "Training Loss 0.036216624081134796\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #78\n",
            "Training Loss 0.036233916878700256\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #79\n",
            "Training Loss 0.036129195243120193\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #80\n",
            "Training Loss 0.03547676280140877\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #81\n",
            "Training Loss 0.035978350788354874\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #82\n",
            "Training Loss 0.03604128211736679\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #83\n",
            "Training Loss 0.03511537238955498\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 226ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #84\n",
            "Training Loss 0.035992395132780075\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #85\n",
            "Training Loss 0.0361427441239357\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #86\n",
            "Training Loss 0.03543717414140701\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #87\n",
            "Training Loss 0.03560597822070122\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #88\n",
            "Training Loss 0.035562753677368164\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #89\n",
            "Training Loss 0.03550012782216072\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #90\n",
            "Training Loss 0.03588210046291351\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #91\n",
            "Training Loss 0.035534851253032684\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #92\n",
            "Training Loss 0.03574185445904732\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #93\n",
            "Training Loss 0.035155653953552246\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #94\n",
            "Training Loss 0.03564875200390816\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #95\n",
            "Training Loss 0.035671696066856384\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #96\n",
            "Training Loss 0.03603890910744667\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #97\n",
            "Training Loss 0.035881493240594864\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #98\n",
            "Training Loss 0.03567750006914139\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #99\n",
            "Training Loss 0.03580622002482414\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #100\n",
            "Training Loss 0.035388778895139694\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #101\n",
            "Training Loss 0.035321831703186035\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #102\n",
            "Training Loss 0.03599189966917038\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #103\n",
            "Training Loss 0.03593733534216881\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #104\n",
            "Training Loss 0.035901833325624466\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #105\n",
            "Training Loss 0.035792551934719086\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #106\n",
            "Training Loss 0.03539275750517845\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #107\n",
            "Training Loss 0.035572245717048645\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #108\n",
            "Training Loss 0.035413797944784164\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #109\n",
            "Training Loss 0.035633038729429245\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #110\n",
            "Training Loss 0.03543093428015709\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #111\n",
            "Training Loss 0.0362430065870285\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #112\n",
            "Training Loss 0.03493775427341461\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #113\n",
            "Training Loss 0.03523201122879982\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #114\n",
            "Training Loss 0.035244170576334\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #115\n",
            "Training Loss 0.03561778739094734\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #116\n",
            "Training Loss 0.035408034920692444\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #117\n",
            "Training Loss 0.03761238232254982\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #118\n",
            "Training Loss 0.044005583971738815\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #119\n",
            "Training Loss 0.043425507843494415\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #120\n",
            "Training Loss 0.03935866802930832\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #121\n",
            "Training Loss 0.037355147302150726\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #122\n",
            "Training Loss 0.035927437245845795\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #123\n",
            "Training Loss 0.03603430464863777\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #124\n",
            "Training Loss 0.0354202538728714\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #125\n",
            "Training Loss 0.035623855888843536\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #126\n",
            "Training Loss 0.035553719848394394\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #127\n",
            "Training Loss 0.03543238341808319\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #128\n",
            "Training Loss 0.035413458943367004\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #129\n",
            "Training Loss 0.03493086248636246\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #130\n",
            "Training Loss 0.034864023327827454\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #131\n",
            "Training Loss 0.03516214340925217\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #132\n",
            "Training Loss 0.03545236960053444\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 215ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #133\n",
            "Training Loss 0.03551231697201729\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #134\n",
            "Training Loss 0.03503410890698433\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #135\n",
            "Training Loss 0.035781633108854294\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #136\n",
            "Training Loss 0.0351831428706646\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #137\n",
            "Training Loss 0.03539304435253143\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #138\n",
            "Training Loss 0.03499045968055725\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #139\n",
            "Training Loss 0.03490608558058739\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #140\n",
            "Training Loss 0.03532358631491661\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #141\n",
            "Training Loss 0.035349126905202866\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #142\n",
            "Training Loss 0.03557693213224411\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #143\n",
            "Training Loss 0.035551972687244415\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #144\n",
            "Training Loss 0.035133685916662216\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #145\n",
            "Training Loss 0.03528923913836479\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 232ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #146\n",
            "Training Loss 0.03516659513115883\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #147\n",
            "Training Loss 0.03550664335489273\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #148\n",
            "Training Loss 0.03530270606279373\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #149\n",
            "Training Loss 0.03552727773785591\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #150\n",
            "Training Loss 0.0355902835726738\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #151\n",
            "Training Loss 0.035518378019332886\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #152\n",
            "Training Loss 0.0358598567545414\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #153\n",
            "Training Loss 0.035645511001348495\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #154\n",
            "Training Loss 0.035393234342336655\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #155\n",
            "Training Loss 0.03520945832133293\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #156\n",
            "Training Loss 0.03591884672641754\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #157\n",
            "Training Loss 0.03580751642584801\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #158\n",
            "Training Loss 0.035341229289770126\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #159\n",
            "Training Loss 0.03541640564799309\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #160\n",
            "Training Loss 0.0350521057844162\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #161\n",
            "Training Loss 0.03538019582629204\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #162\n",
            "Training Loss 0.035513702780008316\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #163\n",
            "Training Loss 0.03543448448181152\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 216ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #164\n",
            "Training Loss 0.03557412326335907\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 217ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #165\n",
            "Training Loss 0.035294000059366226\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #166\n",
            "Training Loss 0.03491031751036644\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #167\n",
            "Training Loss 0.03529494255781174\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 219ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #168\n",
            "Training Loss 0.03541002422571182\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #169\n",
            "Training Loss 0.03531505912542343\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #170\n",
            "Training Loss 0.035475172102451324\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #171\n",
            "Training Loss 0.03582249954342842\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #172\n",
            "Training Loss 0.03481171280145645\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #173\n",
            "Training Loss 0.035499293357133865\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #174\n",
            "Training Loss 0.03568367660045624\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #175\n",
            "Training Loss 0.035399481654167175\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #176\n",
            "Training Loss 0.03505726158618927\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #177\n",
            "Training Loss 0.035641271620988846\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 225ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #178\n",
            "Training Loss 0.03553573042154312\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #179\n",
            "Training Loss 0.03503255546092987\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #180\n",
            "Training Loss 0.035260144621133804\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 220ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #181\n",
            "Training Loss 0.034895025193691254\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #182\n",
            "Training Loss 0.03492041304707527\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 227ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #183\n",
            "Training Loss 0.0352095328271389\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #184\n",
            "Training Loss 0.03529203310608864\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #185\n",
            "Training Loss 0.03564615175127983\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #186\n",
            "Training Loss 0.035249341279268265\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 218ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #187\n",
            "Training Loss 0.03508893772959709\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #188\n",
            "Training Loss 0.034760210663080215\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #189\n",
            "Training Loss 0.03524046763777733\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #190\n",
            "Training Loss 0.035858456045389175\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 225ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #191\n",
            "Training Loss 0.035098347812891006\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 221ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #192\n",
            "Training Loss 0.03503751754760742\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #193\n",
            "Training Loss 0.03529515117406845\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 224ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #194\n",
            "Training Loss 0.03550669923424721\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #195\n",
            "Training Loss 0.03453095629811287\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 225ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #196\n",
            "Training Loss 0.034839704632759094\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 7s 228ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #197\n",
            "Training Loss 0.03487733006477356\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #198\n",
            "Training Loss 0.03449675813317299\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 223ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #199\n",
            "Training Loss 0.03567187115550041\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "29/29 [==============================] - 6s 222ms/step\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "Epoch #200\n",
            "Training Loss 0.035174835473299026\n",
            "<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chatbot(question):\n",
        "  question=clean_text(question)\n",
        "  question=startend(question)\n",
        "\n",
        "  inputs = token_Q.texts_to_sequences([question])\n",
        "  inputs=tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
        "                                                       maxlen=max_sequence_Q,\n",
        "                                                       padding='post')\n",
        "  \n",
        "  hidden = [tf.zeros((1,units))]\n",
        "  out_hidden,  enc_hidden = encoder(inputs,hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "\n",
        "  dec_input=tf.expand_dims([token_a.word_index['<start>']],0)\n",
        "\n",
        "  result=''\n",
        "\n",
        "  for t in range(max_sequence_a):\n",
        "    predictions,dec_hidden = decoder(dec_input,dec_hidden)\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += token_a.index_word[predicted_id] + ' '\n",
        "\n",
        "    if token_a.index_word[predicted_id] == '<end>':\n",
        "      result = result.replace('<start> ', '')\n",
        "      result = result.replace(' <end> ','')\n",
        "      # remove the <start> and <end> tokens from the sentence string\n",
        "      question = question.replace('<start> ', '')\n",
        "      question = question.replace(' <end>', '')\n",
        "      return  question, result\n",
        "\n",
        "    # using the predicted word as the next decoder input\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "  # remove the <start> and <end> tokens from the result string\n",
        "  result = result.replace('<start> ', '')\n",
        "  result = result.replace('<end>','')\n",
        "  # remove the <start> and <end> tokens from the sentence string\n",
        "  question = question.replace('<start> ', '')\n",
        "  question = question.replace('<end>', '')\n",
        "  \n",
        "\n",
        "  return question,result\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Di3lMqU0TLUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chatbot(\"can I have a cup of coffee ?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlbFuR9ALG-C",
        "outputId": "2c685180-62a8-486a-ce4f-28127f6743db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('can i have a cup of coffee', 'sir thanks for sometime')"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot('i will take one espresso and americano')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czDGaA9HLKIt",
        "outputId": "642949e4-41dc-40cc-b960-270f638d3d03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('i will take one espresso and americano', 'sir thanks for sometime')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chatbot('anything special')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvQmN57bLwZB",
        "outputId": "675302de-eada-4674-9520-e73be6c454f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('anything special', 'we have')"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ODnbefS1ML7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.answer[2]"
      ],
      "metadata": {
        "id": "OmqXjkxjMjLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iQMq3RF_U6Y",
        "outputId": "d0452e97-07c0-4ffe-d2f4-141c9a6f6a70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.7/dist-packages (2.8.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.7/dist-packages (from gradio) (0.17.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.7/dist-packages (from gradio) (0.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.5)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.7/dist-packages (from gradio) (0.75.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.7/dist-packages (from gradio) (0.0.5)\n",
            "Requirement already satisfied: paramiko in /usr/local/lib/python3.7/dist-packages (from gradio) (2.10.1)\n",
            "Requirement already satisfied: analytics-python in /usr/local/lib/python3.7/dist-packages (from gradio) (1.4.0)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.7/dist-packages (from gradio) (3.6.7)\n",
            "Requirement already satisfied: markdown-it-py[linkify,plugins] in /usr/local/lib/python3.7/dist-packages (from gradio) (2.0.1)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.7/dist-packages (from gradio) (3.14.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.0.12)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.7.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (21.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (3.10.0.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.2.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.15.0)\n",
            "Requirement already satisfied: python-dateutil>2.1 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (2.8.2)\n",
            "Requirement already satisfied: backoff==1.10.0 in /usr/local/lib/python3.7/dist-packages (from analytics-python->gradio) (1.10.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (2021.10.8)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.7/dist-packages (from fastapi->gradio) (1.9.0)\n",
            "Requirement already satisfied: starlette==0.17.1 in /usr/local/lib/python3.7/dist-packages (from fastapi->gradio) (0.17.1)\n",
            "Requirement already satisfied: anyio<4,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from starlette==0.17.1->fastapi->gradio) (3.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.0.0->starlette==0.17.1->fastapi->gradio) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.1.0)\n",
            "Requirement already satisfied: linkify-it-py~=1.0 in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (1.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.7/dist-packages (from markdown-it-py[linkify,plugins]->gradio) (0.3.0)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.7/dist-packages (from linkify-it-py~=1.0->markdown-it-py[linkify,plugins]->gradio) (1.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2018.9)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (36.0.1)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (3.2.0)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from paramiko->gradio) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko->gradio) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: asgiref>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (3.5.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (0.13.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NFMNPrH0_VH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=chatbot,\n",
        "    inputs=gr.inputs.Textbox(),\n",
        "    outputs=gr.outputs.Textbox(),\n",
        "    title='ChatBot',\n",
        "    description='talk with me here pls',\n",
        "    theme='dark',\n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "_qJZQcgFVyc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "dd5b5ff8-7899-4dac-ee63-6e6bbd2e67d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://36185.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fc0c6755dd0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://36185.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<fastapi.applications.FastAPI at 0x7fc205a25ed0>,\n",
              " 'http://127.0.0.1:7860/',\n",
              " 'https://36185.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message, history):\n",
        "    obj=chatbot(message)\n",
        "    history = history or []\n",
        "  \n",
        "    \n",
        "    history.append((obj))\n",
        "    return history, history\n",
        "\n",
        "\n",
        "iface = gr.Interface(\n",
        "    chat,\n",
        "    [\"text\", \"state\"],\n",
        "    [\"chatbot\", \"state\"],\n",
        "    allow_screenshot=False,\n",
        "    allow_flagging=\"never\",\n",
        "    \n",
        ")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "uWLUSgXz_u9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "b23aa5d1-9a1a-4c0d-cc48-716522ed18dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://37775.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7fc0ca50bb50>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://37775.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<fastapi.applications.FastAPI at 0x7fc205a25ed0>,\n",
              " 'http://127.0.0.1:7862/',\n",
              " 'https://37775.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CxA1Jwj1PmIL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}